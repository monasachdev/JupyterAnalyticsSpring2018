{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "PSY 394U <b>Data Analytics with Python</b>, Spring 2018\n",
    "\n",
    "\n",
    "<img style=\"width: 400px; padding: 0px;\" src=\"https://github.com/sathayas/JupyterAnalyticsSpring2018/blob/master/images/Title_pics.png?raw=true\" alt=\"title pics\"/>\n",
    "\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:center; font-size:40px; margin-bottom: 30px;\"><b> Text classification </b></p>\n",
    "\n",
    "<p style=\"text-align:center; font-size:18px; margin-bottom: 32px;\"><b>April 24 - 26, 2018</b></p>\n",
    "\n",
    "<hr style=\"height:5px;border:none\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data can be analyzed by classification and clustering algorithms. This can be done by extracting features from a text data corpus, and performing a classification or clustering according to the extracted features and the target category data. Here, we cover a few simple examples of text classification and clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. String classification\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "The goal here is to classify which strings belong to which category. To do so, we will use the corpus **`name`**, a collection of female and male names on NLTK, and construct a classifier to determine whether a name is of a female or a male."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<NameClassifier.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "\n",
    "# reading names from the names corpus\n",
    "from nltk.corpus import names\n",
    "femaleNames = names.words('female.txt')\n",
    "maleNames = names.words('male.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are reading in names of females and males. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating name-label pairs, then shuffling\n",
    "nameData = []\n",
    "for iName in femaleNames:\n",
    "    nameData.append((iName, 'female'))\n",
    "for iName in maleNames:\n",
    "    nameData.append((iName, 'male'))\n",
    "random.shuffle(nameData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once both data are read, we combine the name with its category ('female' or 'male') as tuples. Then we shuffle the data so that male and female names are now mixed.\n",
    "\n",
    "To extract a feature from the shuffled data file, we will use a custom function called **`gender_feature`**. This function takes a string, then returns a feature (in this example, the last letter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to return a feature to classify whether a name is\n",
    "# male or female.\n",
    "# The feature and the label are returned together\n",
    "def gender_feature(name):\n",
    "    featureDict = {'last-letter': name[-1]}\n",
    "    return featureDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we convert the list of names (**`nameData`**) into a list of features (**`last-letter`**). Again, we shall keep both the feature dictionary and the label in **`featureData`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the name data into feature (i.e., just the last letter)\n",
    "# as well as the label (female / male)\n",
    "featureData = [(gender_feature(n), gender) for (n, gender) in nameData]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are generating training and testing data sets, with the testing data set comprising 1000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting into training and testing data sets\n",
    "trainData, testData = featureData[1000:], featureData[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train a classifier. Here, we use a naive Bayes algorithm. A naive Bayes classifier classifies observations as the most likely outcomes based on the Bayes theory (i.e., the distribution of the label given the observed feature(s)). Naive Bayes classifiers are widely used in text classification, such as spam detection and sentiment analysis. The naive Bayes classifier is available in NLTK as **`NaiveBayesClassifier`**. This classifier object is somewhat different from that of **Scikit-learn** (a.k.a., `sklearn`). We supply both feature(s) and category label to the naive Bayes algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a classifier (Naive Bayes)\n",
    "clf = nltk.NaiveBayesClassifier.train(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the classifier has been trained. Let's see how well it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n"
     ]
    }
   ],
   "source": [
    "# classification example\n",
    "print(clf.classify(gender_feature('Nemo')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female\n"
     ]
    }
   ],
   "source": [
    "print(clf.classify(gender_feature('Dory')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we shall see how the classifier performs on our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765\n"
     ]
    }
   ],
   "source": [
    "# classifier performance on the testing data\n",
    "print(nltk.classify.accuracy(clf, testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually a good performance in terms of accuracy. Unlike classifiers in `sklearn`, the naive Bayes classifier in NLTK lets you examine the most informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last-letter = 'k'              male : female =     40.5 : 1.0\n",
      "             last-letter = 'a'            female : male   =     35.8 : 1.0\n",
      "             last-letter = 'f'              male : female =     14.6 : 1.0\n",
      "             last-letter = 'p'              male : female =     11.2 : 1.0\n",
      "             last-letter = 'd'              male : female =     10.1 : 1.0\n",
      "             last-letter = 'v'              male : female =      9.2 : 1.0\n",
      "             last-letter = 'o'              male : female =      8.5 : 1.0\n",
      "             last-letter = 'm'              male : female =      8.1 : 1.0\n",
      "             last-letter = 'u'              male : female =      7.8 : 1.0\n",
      "             last-letter = 'g'              male : female =      6.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# most informative features\n",
    "clf.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it seems like a name ending with \"a\" is likely classified as a female name.\n",
    "\n",
    "How can we improve the performance of the classifier? Let's examine misclassified cases and see if there is any pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examining classification errors\n",
    "errorData = []\n",
    "testDataFull = nameData[:1000]\n",
    "for iData in testDataFull:\n",
    "    trueCat = iData[1]\n",
    "    predCat = clf.classify(gender_feature(iData[0]))\n",
    "    if predCat != trueCat:\n",
    "        errorData.append((trueCat, predCat, iData[0]))\n",
    "\n",
    "\n",
    "# printing out the errors\n",
    "for (y_true, y_pred, name) in sorted(errorData):\n",
    "    print('Truth: %-6s\\t' % y_true, end='')\n",
    "    print('Pred: %-6s\\t' % y_pred, end='')\n",
    "    print('Name: %-12s' % name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I will not print out all the misclassified cases)\n",
    "\n",
    "Notice that there are some patterns. For example,\n",
    "```\n",
    "...\n",
    "Truth: female\tPred: male  \tName: Catlin      \n",
    "...\n",
    "Truth: female\tPred: male  \tName: Christin    \n",
    "...\n",
    "Truth: female\tPred: male  \tName: Dyann       \n",
    "Truth: female\tPred: male  \tName: Emlynn      \n",
    "...\n",
    "Truth: female\tPred: male  \tName: Jacquelin   \n",
    "...\n",
    "Truth: female\tPred: male  \tName: Joann       \n",
    "Truth: female\tPred: male  \tName: Joycelin    \n",
    "...\n",
    "Truth: female\tPred: male  \tName: Kerstin     \n",
    "...\n",
    "```\n",
    "Notice that some misclassified female names end with \"in\" and \"nn.\" Moreover,\n",
    "```\n",
    "...\n",
    "Truth: male  \tPred: female\tName: Artie       \n",
    "...\n",
    "Truth: male  \tPred: female\tName: Benjie      \n",
    "...\n",
    "Truth: male  \tPred: female\tName: Bobbie      \n",
    "...\n",
    "Truth: male  \tPred: female\tName: Eddie       \n",
    "...\n",
    "Truth: male  \tPred: female\tName: Ricky       \n",
    "Truth: male  \tPred: female\tName: Rocky       \n",
    "...\n",
    "Truth: male  \tPred: female\tName: Sparky      \n",
    "...\n",
    "Truth: male  \tPred: female\tName: Tucky       \n",
    "...\n",
    "```\n",
    "Some misclassified male names end with \"ie\" and \"ky\". So, in addition to the last letter, we can use the last two letters as another feature to improve the classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<NameClassifierRevised.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to return a feature to classify whether a name is\n",
    "# male or female.\n",
    "# The feature and the label are returned together\n",
    "def gender_feature(name):\n",
    "    featureDict = {'last-letter': name[-1], 'last2': name[-2:]}\n",
    "    return featureDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are two features in the feature dictionary: **`last-letter`** and **`last2`**. Let's use these features and re-classify the name data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792\n"
     ]
    }
   ],
   "source": [
    "# converting the name data into features \n",
    "# as well as the label (female / male)\n",
    "featureData = [(gender_feature(n), gender) for (n, gender) in nameData]\n",
    "\n",
    "# spliting into training and testing data sets\n",
    "trainData, testData = featureData[1000:], featureData[:1000]\n",
    "\n",
    "# training a classifier (Naive Bayes)\n",
    "clf = nltk.NaiveBayesClassifier.train(trainData)\n",
    "\n",
    "# classifier performance on the testing data\n",
    "print(nltk.classify.accuracy(clf, testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is some improvement in the performance. Here are most informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   last2 = 'na'           female : male   =     96.1 : 1.0\n",
      "                   last2 = 'la'           female : male   =     67.0 : 1.0\n",
      "             last-letter = 'k'              male : female =     40.5 : 1.0\n",
      "                   last2 = 'ia'           female : male   =     36.9 : 1.0\n",
      "             last-letter = 'a'            female : male   =     35.8 : 1.0\n",
      "                   last2 = 'ra'           female : male   =     33.2 : 1.0\n",
      "                   last2 = 'ld'             male : female =     32.8 : 1.0\n",
      "                   last2 = 'sa'           female : male   =     32.6 : 1.0\n",
      "                   last2 = 'ta'           female : male   =     30.2 : 1.0\n",
      "                   last2 = 'rt'             male : female =     28.3 : 1.0\n",
      "                   last2 = 'us'             male : female =     26.0 : 1.0\n",
      "                   last2 = 'rd'             male : female =     25.0 : 1.0\n",
      "                   last2 = 'io'             male : female =     23.9 : 1.0\n",
      "                   last2 = 'os'             male : female =     20.6 : 1.0\n",
      "                   last2 = 'im'             male : female =     15.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# most informative features\n",
    "clf.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the majority of informative features are the last two letters.\n",
    "\n",
    "### Exercise\n",
    "1. **Last 3 letters**. Revise the code above by adding an additional feature, the last 3 letters. Then re-run the classifier. Print out 15 most informative features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Document classification\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "# Document classification with NLTK\n",
    "In addition to classification of string data, you can also classify documents using NLTK. Such document classification is often done by using a **bag-of-words** approach. A bag-of-words is a list of words used in a document. Here, we ignore word order or any grammatical structure. We solely focus on which words are used in a document, and with a sufficient number of documents in a corpus, we can build a classifier to categorize a document. \n",
    "\n",
    "In this example, we will use the **`movie-reviews`** corpus from NLTK, one of the example corpora we have been using. This type of classification can be used in a **sentiment analysis**, to infer the sentiment of the writer (positive or negative, in this particular case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<DocClassify.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# creating a list of document-label pairs\n",
    "from nltk.corpus import movie_reviews as mr\n",
    "reviewList = []\n",
    "for iCat in mr.categories():  # first, going over categories (pos or neg)\n",
    "    for iReview in mr.fileids([iCat]):   # reviews in that category\n",
    "        reviewPair = (mr.words(iReview), iCat)\n",
    "        reviewList.append(reviewPair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read documents in the corpus, and create a list of document-label pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling, and separating into testing and training data sets\n",
    "random.shuffle(reviewList)\n",
    "trainList, testList = reviewList[500:], reviewList[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the reviews are grouped by categories (pos or neg), so we shuffle them first, then split them into the testing (500 reviews) and training (all the rest) data sets. Next, we create a list of 2000 most frequently appearing words in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of all words in the training data set\n",
    "allWords = []\n",
    "for iReviewPair in trainList:\n",
    "    reviewWords = [w.lower() for w in iReviewPair[0]]\n",
    "    # Just in case someone writes a review IN ALL CAPS\n",
    "    allWords += reviewWords\n",
    "\n",
    "\n",
    "# word frequency, and just consider 2000 most frequent words\n",
    "allWordFreq = nltk.FreqDist(allWords)\n",
    "featureWords = [w for (w,c) in allWordFreq.most_common(2000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the most frequently used words are punctuations and stop words. But since there are so many words in the feature word list, we do not worry about those. At this point, we define a function to extract a bag-of-words from each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document features (whether contains certain words)\n",
    "def document_features(document): \n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for w in featureWords:\n",
    "        features['contains({})'.format(w)] = (w in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a dictionary of features, whether each of the 2000 words is contained in the document. We use this function to extract features from each document in the training and testing data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting features for training and testing data\n",
    "trainSet = [(document_features(d), c) for (d,c) in trainList]\n",
    "testSet = [(document_features(d), c) for (d,c) in testList]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry in the training and testing data set is a dictionary of features, i.e., whether a certain word is contained in the document, as well as the target class ('pos' or 'neg').\n",
    "```\n",
    "({'contains(surprise)': False,\n",
    "  'contains(version)': False,\n",
    "  'contains(lady)': False,\n",
    "  'contains(constantly)': False,\n",
    "  'contains(minute)': True,\n",
    "  'contains(sheer)': False,\n",
    "  'contains(memorable)': False,\n",
    "  'contains(hospital)': False,\n",
    "  'contains(himself)': False,\n",
    "  ...},\n",
    " 'pos')\n",
    "```\n",
    "\n",
    "Now, we are ready to run the naive Bayes classifier. *Please note that it may take a few minutes to run the classifier*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828\n"
     ]
    }
   ],
   "source": [
    "# classifier\n",
    "clf = nltk.NaiveBayesClassifier.train(trainSet)\n",
    "print(nltk.classify.accuracy(clf, testSet)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the classifier did a fairly good job in classifying the review sentiment. The most informative features are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "        contains(seagal) = True              neg : pos    =     11.0 : 1.0\n",
      "         contains(mulan) = True              pos : neg    =      8.3 : 1.0\n",
      "   contains(wonderfully) = True              pos : neg    =      8.3 : 1.0\n",
      "         contains(awful) = True              neg : pos    =      6.0 : 1.0\n",
      "          contains(lame) = True              neg : pos    =      5.3 : 1.0\n",
      "        contains(wasted) = True              neg : pos    =      5.1 : 1.0\n",
      "          contains(jedi) = True              pos : neg    =      5.0 : 1.0\n",
      "         contains(waste) = True              neg : pos    =      4.9 : 1.0\n",
      "       contains(freedom) = True              pos : neg    =      4.7 : 1.0\n",
      "         contains(worst) = True              neg : pos    =      4.5 : 1.0\n",
      "    contains(ridiculous) = True              neg : pos    =      4.5 : 1.0\n",
      "        contains(poorly) = True              neg : pos    =      4.2 : 1.0\n",
      "        contains(superb) = True              pos : neg    =      4.2 : 1.0\n",
      "     contains(laughable) = True              neg : pos    =      4.1 : 1.0\n",
      "           contains(era) = True              pos : neg    =      4.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# most informative features\n",
    "clf.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a classifier from `sklearn` in NLTK\n",
    "\n",
    "Besides the naive Bayes classifier, you can also use a classifier available in `sklearn` in NLTK. This is done by a *wrapper* available in NLTK. A wrapper is a function that calls one function from another function.  So, let's say we want to use a support vector machine (SVM) from `sklearn`. In that case, we need to import **`SklearnClassifier`** from **`nltk.classify.scikitlearn`**, a wrapper utility enabling a use of `sklearn` classifier in NLTK. In addition, we also need to import the SVM classifier in `sklearn`. We will use **`LinearSVC`**, a SVM with a linear kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to use a linear SVM for our movie review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782\n"
     ]
    }
   ],
   "source": [
    "# Now with SVM classifier (linear kernel)\n",
    "clf_svm = SklearnClassifier(LinearSVC())\n",
    "clf_svm.train(trainSet)\n",
    "print(nltk.classify.accuracy(clf_svm, testSet)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, since this is a classifier from `sklearn`, not from NLTK, so it does not let you print out most informative features.\n",
    "\n",
    "### Exercise\n",
    "1. **Reviewer sentiment, same or different?**. The program **`SentimentClassify.py`** (available on GitHub) classifies user reviews on Amazon.com as positive (1) or negative (0), using the naive Bayes classifier in NLTK. You want to see if the classifier for the reviewer sentiment for Amazon works on reviews from other platforms. In the directory **`SentimentReviews`** (available on GitHub) are labeled reviews from IMDB (**`imdb_labelled.txt`**) and Yelp (**`yelp_labelled.txt`**). Chose either one of the review corpora, and classify the reviews using the classifier from `SentimentClassify.py` based on Amazon reviews. Calculate the classification accuracy.\n",
    "2. **Sanity check**. Modify the program **`SentimentClassify.py`** so that it can construct a classifier based on the review corpus you chose for the earlier exercise. Calculate the classification accuracy, and print 15 most informative features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document classification with `sklearn`\n",
    "\n",
    "NLTK is great for processing text data. However, its classification functionality is not as extensive as other libraries specialized in machine learning. Luckily, we can perform document classification using **`sklearn`**. The classification tools available in `sklearn` has been optimized and run much faster than that of NLTK. We will revisit the sentiment analysis of movie reviews earlier using `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<DocClassifyFreq.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# creating a list of document-label pairs\n",
    "from nltk.corpus import movie_reviews as mr\n",
    "reviewList = []\n",
    "for iCat in mr.categories():  # first, going over categories (pos or neg)\n",
    "    for iReview in mr.fileids([iCat]):   # reviews in that category\n",
    "        reviewPair = (mr.raw(iReview), iCat)\n",
    "        reviewList.append(reviewPair)\n",
    "\n",
    "# splitting into training and testing data\n",
    "X = [d for (d, c) in reviewList]\n",
    "Y = [c for (d, c) in reviewList]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    test_size=500,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we load the data, and split into the training and testing data using `sklearn`'s **`train_test_split`** function. \n",
    "\n",
    "Next, we generate a bag-of-words for each review. To do so, we use the **`CountVectorizer`** transformation object available in **`sklearn.feature_extraction.text`**. `CountVectorizer` tokenizes a document into words, changes to lower case, and removes punctuations and stop words, all in one step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word occurrence counts\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation object **`count_vect`** in this case returns word frequency counts in all documents in the training data `X_train`. You can examine the list of all words in the training data corpus by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words\n",
    "#count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`X_train_counts`** is a matrix of word occurrence counts, rows corresponding to documents in the training data, and the columns corresponding to the number of unique words appearing in the training data corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 35321)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the majority of the elements in `X_train_counts` are zeros, thus it is stored as a *sparse matrix* as opposed to a 2D array. You can still examine the elements of `X_train_counts` by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    0, ..., 1499, 1499, 1499], dtype=int32),\n",
       " array([ 8406, 14937,  1835, ..., 12551,  9283, 34287], dtype=int32))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indices for non-zero elements in the sparse matrix\n",
    "X_train_counts.nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to convert the word occurrence counts to word frequencies. This is because the word occurrence counts depend highly on the length of a document. For example, the word \"absolutely\" may appear only 3 times in a 2-page essay, but a 500-page novel may contain 232 uses of \"absolutely.\" The **`TfidfTransformer`** transformer object in **`sklearn.feature_extraction.text`** can transform word count data into word frequency (known as **term frequency**, or **tf**) data. In addition to converting counts to frequencies, it also down-weight words that are abundant in the corpus. This is because words commonly used in a corpus add very little information for classification. This process of down-weighting is known as **inverse document frequency** or **idf**. `TfidfTransformer` can covert count data to **tf-idf** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to term frequency\n",
    "tf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **`X_train_tf`** as the features in a classifier. Here, we use a naive Bayes classifier again. Here, we use the **`MultinomialNB`** classifier object, a multinomial naive Bayes classifier, available in **`sklearn.naive_bayes`**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier (naive Bayes)\n",
    "clf_nb = MultinomialNB().fit(X_train_tf, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the classifier has been trained, so we now classify the testing data. It needs to be converted to the count data, then converted to the frequency data. We have to use **`count_vect`** and **`tf_transformer`**, respectively, already fitted for our training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the testing set to term frequency\n",
    "X_test_counts = count_vect.transform(X_test)  # NB you don't have to fit\n",
    "X_test_tf = tf_transformer.transform(X_test_counts)  # NB you don't have to fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally classifying **`X_test_tf`**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifying the testing data\n",
    "Y_pred_nb = clf_nb.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the classifier performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - Naive Bayes: 0.8260\n",
      "[[219  35]\n",
      " [ 52 194]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.81      0.86      0.83       254\n",
      "        pos       0.85      0.79      0.82       246\n",
      "\n",
      "avg / total       0.83      0.83      0.83       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "print('Accuracy - Naive Bayes: %6.4f' % accuracy_score(Y_test,Y_pred_nb))\n",
    "print(confusion_matrix(Y_test,Y_pred_nb))\n",
    "print(classification_report(Y_test,Y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, we can try a linear SVM as a classifier on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - Linear SVM: 0.8760\n",
      "[[218  36]\n",
      " [ 26 220]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.89      0.86      0.88       254\n",
      "        pos       0.86      0.89      0.88       246\n",
      "\n",
      "avg / total       0.88      0.88      0.88       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classifier (Linear SVM)\n",
    "clf_svm = LinearSVC().fit(X_train_tf, Y_train)\n",
    "\n",
    "# classifying the testing data\n",
    "Y_pred_svm = clf_svm.predict(X_test_tf)\n",
    "\n",
    "# accuracy\n",
    "print('Accuracy - Linear SVM: %6.4f' % accuracy_score(Y_test,Y_pred_svm))\n",
    "print(confusion_matrix(Y_test,Y_pred_svm))\n",
    "print(classification_report(Y_test,Y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Name classification\n",
    "   * Exercise: last 3 letters, first letter vowel\n",
    "* Text classification\n",
    "   * NLTK classifier (Naive Bayes, sklearn wrapper)\n",
    "       * Exercise: SentimentReview\n",
    "   * sklearn tools\n",
    "   * (cross validation)\n",
    "       * Exercise: News groups\n",
    "* Text clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
